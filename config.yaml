# =============================================================================
# YantraOS — Configuration Template
# =============================================================================
# Copy this file to /opt/yantra/config.yaml and fill in your API keys.
# Permissions: root:yantra 640 (owner rw, group r)
# =============================================================================

# ─── DAEMON SETTINGS ─────────────────────────────────────────────────────────
daemon:
  name: "yantra-daemon"
  version: "0.1.0"
  loop_interval_sec: 10          # Sleep between Kriya Loop iterations
  watchdog_interval_sec: 15      # Must match WatchdogSec in yantra.service
  log_level: "INFO"              # DEBUG | INFO | WARNING | ERROR
  log_file: "/var/log/yantra/engine.log"
  max_log_size_mb: 50
  log_rotation_count: 5

# ─── HARDWARE THRESHOLDS ────────────────────────────────────────────────────
hardware:
  vram_local_threshold_gb: 16    # >= this = LOCAL_CAPABLE
  vram_minimum_gb: 8             # >= this = usable for smaller models
  gpu_poll_interval_sec: 2       # nvidia-smi polling frequency

# ─── INFERENCE ROUTING ───────────────────────────────────────────────────────
inference:
  # Local inference (requires LOCAL_CAPABLE hardware)
  local:
    provider: "ollama"
    base_url: "http://localhost:11434"
    models:
      - "ollama/llama3"
      - "ollama/deepseek-r1"
    timeout_sec: 120

  # Cloud inference (fallback or CLOUD_ONLY mode)
  cloud:
    primary:
      provider: "gemini"
      model: "gemini/gemini-2.5-flash"
      api_key: "${GOOGLE_GENERATIVE_AI_API_KEY}"  # Set via env or .env file
      timeout_sec: 60
    fallback:
      provider: "anthropic"
      model: "claude/claude-3.5-haiku"
      api_key: "${ANTHROPIC_API_KEY}"             # Set via env or .env file
      timeout_sec: 60

# ─── VECTOR MEMORY (ChromaDB) ───────────────────────────────────────────────
memory:
  provider: "chromadb"
  persist_directory: "/var/lib/yantra/chroma"
  collection_name: "yantra_executions"
  embedding_model: "all-MiniLM-L6-v2"
  similarity_threshold: 0.85     # Minimum cosine similarity for RAG recall
  max_results: 5

# ─── PINECONE (Cloud Sync) ──────────────────────────────────────────────────
pinecone:
  api_key: "${PINECONE_API_KEY}"
  index_name: "yantra-skills"
  namespace_strategy: "per-skill" # One namespace per skill slug
  dimensions: 1536
  metric: "cosine"

# ─── DOCKER SANDBOX ─────────────────────────────────────────────────────────
sandbox:
  image: "yantra-sandbox:latest"
  timeout_sec: 30
  memory_limit: "256m"
  cpu_quota: 50                  # Percentage of one CPU core
  network: "none"                # Complete network isolation
  max_output_bytes: 1048576      # 1MB stdout/stderr cap

# ─── IPC (Inter-Process Communication) ──────────────────────────────────────
ipc:
  socket_path: "/run/yantra/ipc.sock"
  buffer_size: 65536

# ─── TELEMETRY ───────────────────────────────────────────────────────────────
telemetry:
  enabled: true
  emit_interval_sec: 5
  targets:
    - type: "journal"            # systemd journalctl
    - type: "websocket"          # Push to Web HUD
      url: "wss://yantraos.com/api/telemetry/ws"
    - type: "http"               # POST to Vercel backend
      url: "https://yantraos.com/api/telemetry/ingest"
    - type: "ipc"                # Local Yantra Shell TUI
